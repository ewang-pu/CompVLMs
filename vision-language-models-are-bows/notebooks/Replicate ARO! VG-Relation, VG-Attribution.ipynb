{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35150174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from model_zoo import get_model\n",
    "from dataset_zoo import VG_Relation, VG_Attribution\n",
    "\n",
    "from transformers import ViltProcessor, ViltForImageAndTextRetrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0094edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vilt_processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-coco\")\n",
    "vilt_model = ViltForImageAndTextRetrieval.from_pretrained(\"dandelin/vilt-b32-finetuned-coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02d2d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please put your data root directory below. We'll download VG-Relation and VG-Attribution images here. \n",
    "# Will be a 1GB zip file (a subset of GQA).\n",
    "root_dir=\"C:/Users/ewang/OneDrive/Desktop/Fall 2023/CompVLMs/vision-language-models-are-bows/data2\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc4eb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = get_model(model_name=\"blip-flickr-base\", device=\"cuda\", root_dir=root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44bf8eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = get_model(model_name=\"dandelin/vilt-b32-finetuned-coco\", device=\"cuda\", root_dir=root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b942fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing retrieval scores:   0%|          | 0/24 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The image to be converted to a PIL image contains values outside the range [0, 1], got [-1.7922625541687012, 2.1458969116210938] which cannot be converted to uint8.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ewang\\OneDrive\\Desktop\\Fall 2023\\CompVLMs\\vision-language-models-are-bows\\notebooks\\Replicate ARO! VG-Relation, VG-Attribution.ipynb Cell 6\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ewang/OneDrive/Desktop/Fall%202023/CompVLMs/vision-language-models-are-bows/notebooks/Replicate%20ARO%21%20VG-Relation%2C%20VG-Attribution.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m vgr_loader \u001b[39m=\u001b[39m DataLoader(vgr_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ewang/OneDrive/Desktop/Fall%202023/CompVLMs/vision-language-models-are-bows/notebooks/Replicate%20ARO%21%20VG-Relation%2C%20VG-Attribution.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Compute the scores for each test case\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ewang/OneDrive/Desktop/Fall%202023/CompVLMs/vision-language-models-are-bows/notebooks/Replicate%20ARO%21%20VG-Relation%2C%20VG-Attribution.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m vgr_scores \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_retrieval_scores_batched(vgr_loader)\n",
      "File \u001b[1;32mc:\\Users\\ewang\\anaconda3\\envs\\CompIW\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ewang\\OneDrive\\Desktop\\Fall 2023\\CompVLMs\\vision-language-models-are-bows\\notebooks\\..\\model_zoo\\vilt_models.py:48\u001b[0m, in \u001b[0;36mViLTWrapper.get_retrieval_scores_batched\u001b[1;34m(self, joint_loader)\u001b[0m\n\u001b[0;32m     46\u001b[0m c_options1 \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mcaption_options\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m]\n\u001b[0;32m     47\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(i_options)):\n\u001b[1;32m---> 48\u001b[0m     inputs0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessor(\n\u001b[0;32m     49\u001b[0m         images\u001b[39m=\u001b[39mi_options[i],\n\u001b[0;32m     50\u001b[0m         text\u001b[39m=\u001b[39mc_options0[i],\n\u001b[0;32m     51\u001b[0m         return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     52\u001b[0m         padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     53\u001b[0m         truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     54\u001b[0m     )\n\u001b[0;32m     55\u001b[0m     inputs1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessor(\n\u001b[0;32m     56\u001b[0m         images\u001b[39m=\u001b[39mi_options[i],\n\u001b[0;32m     57\u001b[0m         text\u001b[39m=\u001b[39mc_options1[i],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m         truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     61\u001b[0m     )\n\u001b[0;32m     63\u001b[0m     outputs0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs0)\n",
      "File \u001b[1;32mc:\\Users\\ewang\\anaconda3\\envs\\CompIW\\Lib\\site-packages\\transformers\\models\\vilt\\processing_vilt.py:108\u001b[0m, in \u001b[0;36mViltProcessor.__call__\u001b[1;34m(self, images, text, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, return_tensors, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(\n\u001b[0;32m     90\u001b[0m     text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m     91\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    106\u001b[0m )\n\u001b[0;32m    107\u001b[0m \u001b[39m# add pixel_values + pixel_mask\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m encoding_image_processor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_processor(images, return_tensors\u001b[39m=\u001b[39mreturn_tensors)\n\u001b[0;32m    109\u001b[0m encoding\u001b[39m.\u001b[39mupdate(encoding_image_processor)\n\u001b[0;32m    111\u001b[0m \u001b[39mreturn\u001b[39;00m encoding\n",
      "File \u001b[1;32mc:\\Users\\ewang\\anaconda3\\envs\\CompIW\\Lib\\site-packages\\transformers\\image_processing_utils.py:546\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[1;34m(self, images, **kwargs)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, images, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchFeature:\n\u001b[0;32m    545\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 546\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(images, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ewang\\anaconda3\\envs\\CompIW\\Lib\\site-packages\\transformers\\models\\vilt\\image_processing_vilt.py:441\u001b[0m, in \u001b[0;36mViltImageProcessor.preprocess\u001b[1;34m(self, images, do_resize, size, size_divisor, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_pad, return_tensors, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[0;32m    438\u001b[0m     input_data_format \u001b[39m=\u001b[39m infer_channel_dimension_format(images[\u001b[39m0\u001b[39m])\n\u001b[0;32m    440\u001b[0m \u001b[39mif\u001b[39;00m do_resize:\n\u001b[1;32m--> 441\u001b[0m     images \u001b[39m=\u001b[39m [\n\u001b[0;32m    442\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresize(\n\u001b[0;32m    443\u001b[0m             image\u001b[39m=\u001b[39mimage,\n\u001b[0;32m    444\u001b[0m             size\u001b[39m=\u001b[39msize,\n\u001b[0;32m    445\u001b[0m             size_divisor\u001b[39m=\u001b[39msize_divisor,\n\u001b[0;32m    446\u001b[0m             resample\u001b[39m=\u001b[39mresample,\n\u001b[0;32m    447\u001b[0m             input_data_format\u001b[39m=\u001b[39minput_data_format,\n\u001b[0;32m    448\u001b[0m         )\n\u001b[0;32m    449\u001b[0m         \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images\n\u001b[0;32m    450\u001b[0m     ]\n\u001b[0;32m    452\u001b[0m \u001b[39mif\u001b[39;00m do_rescale:\n\u001b[0;32m    453\u001b[0m     images \u001b[39m=\u001b[39m [\n\u001b[0;32m    454\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrescale(image\u001b[39m=\u001b[39mimage, scale\u001b[39m=\u001b[39mrescale_factor, input_data_format\u001b[39m=\u001b[39minput_data_format)\n\u001b[0;32m    455\u001b[0m         \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images\n\u001b[0;32m    456\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\ewang\\anaconda3\\envs\\CompIW\\Lib\\site-packages\\transformers\\models\\vilt\\image_processing_vilt.py:442\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    438\u001b[0m     input_data_format \u001b[39m=\u001b[39m infer_channel_dimension_format(images[\u001b[39m0\u001b[39m])\n\u001b[0;32m    440\u001b[0m \u001b[39mif\u001b[39;00m do_resize:\n\u001b[0;32m    441\u001b[0m     images \u001b[39m=\u001b[39m [\n\u001b[1;32m--> 442\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresize(\n\u001b[0;32m    443\u001b[0m             image\u001b[39m=\u001b[39mimage,\n\u001b[0;32m    444\u001b[0m             size\u001b[39m=\u001b[39msize,\n\u001b[0;32m    445\u001b[0m             size_divisor\u001b[39m=\u001b[39msize_divisor,\n\u001b[0;32m    446\u001b[0m             resample\u001b[39m=\u001b[39mresample,\n\u001b[0;32m    447\u001b[0m             input_data_format\u001b[39m=\u001b[39minput_data_format,\n\u001b[0;32m    448\u001b[0m         )\n\u001b[0;32m    449\u001b[0m         \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images\n\u001b[0;32m    450\u001b[0m     ]\n\u001b[0;32m    452\u001b[0m \u001b[39mif\u001b[39;00m do_rescale:\n\u001b[0;32m    453\u001b[0m     images \u001b[39m=\u001b[39m [\n\u001b[0;32m    454\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrescale(image\u001b[39m=\u001b[39mimage, scale\u001b[39m=\u001b[39mrescale_factor, input_data_format\u001b[39m=\u001b[39minput_data_format)\n\u001b[0;32m    455\u001b[0m         \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images\n\u001b[0;32m    456\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\ewang\\anaconda3\\envs\\CompIW\\Lib\\site-packages\\transformers\\models\\vilt\\image_processing_vilt.py:244\u001b[0m, in \u001b[0;36mViltImageProcessor.resize\u001b[1;34m(self, image, size, size_divisor, resample, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[0;32m    240\u001b[0m longer \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m1333\u001b[39m \u001b[39m/\u001b[39m \u001b[39m800\u001b[39m \u001b[39m*\u001b[39m shorter)\n\u001b[0;32m    241\u001b[0m output_size \u001b[39m=\u001b[39m get_resize_output_image_size(\n\u001b[0;32m    242\u001b[0m     image, shorter\u001b[39m=\u001b[39mshorter, longer\u001b[39m=\u001b[39mlonger, size_divisor\u001b[39m=\u001b[39msize_divisor, input_data_format\u001b[39m=\u001b[39minput_data_format\n\u001b[0;32m    243\u001b[0m )\n\u001b[1;32m--> 244\u001b[0m \u001b[39mreturn\u001b[39;00m resize(\n\u001b[0;32m    245\u001b[0m     image,\n\u001b[0;32m    246\u001b[0m     size\u001b[39m=\u001b[39moutput_size,\n\u001b[0;32m    247\u001b[0m     resample\u001b[39m=\u001b[39mresample,\n\u001b[0;32m    248\u001b[0m     data_format\u001b[39m=\u001b[39mdata_format,\n\u001b[0;32m    249\u001b[0m     input_data_format\u001b[39m=\u001b[39minput_data_format,\n\u001b[0;32m    250\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    251\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ewang\\anaconda3\\envs\\CompIW\\Lib\\site-packages\\transformers\\image_transforms.py:326\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(image, size, resample, reducing_gap, data_format, return_numpy, input_data_format)\u001b[0m\n\u001b[0;32m    324\u001b[0m do_rescale \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(image, PIL\u001b[39m.\u001b[39mImage\u001b[39m.\u001b[39mImage):\n\u001b[1;32m--> 326\u001b[0m     do_rescale \u001b[39m=\u001b[39m _rescale_for_pil_conversion(image)\n\u001b[0;32m    327\u001b[0m     image \u001b[39m=\u001b[39m to_pil_image(image, do_rescale\u001b[39m=\u001b[39mdo_rescale, input_data_format\u001b[39m=\u001b[39minput_data_format)\n\u001b[0;32m    328\u001b[0m height, width \u001b[39m=\u001b[39m size\n",
      "File \u001b[1;32mc:\\Users\\ewang\\anaconda3\\envs\\CompIW\\Lib\\site-packages\\transformers\\image_transforms.py:150\u001b[0m, in \u001b[0;36m_rescale_for_pil_conversion\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m    148\u001b[0m     do_rescale \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    151\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe image to be converted to a PIL image contains values outside the range [0, 1], \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    152\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgot [\u001b[39m\u001b[39m{\u001b[39;00mimage\u001b[39m.\u001b[39mmin()\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mimage\u001b[39m.\u001b[39mmax()\u001b[39m}\u001b[39;00m\u001b[39m] which cannot be converted to uint8.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    153\u001b[0m     )\n\u001b[0;32m    154\u001b[0m \u001b[39mreturn\u001b[39;00m do_rescale\n",
      "\u001b[1;31mValueError\u001b[0m: The image to be converted to a PIL image contains values outside the range [0, 1], got [-1.7922625541687012, 2.1458969116210938] which cannot be converted to uint8."
     ]
    }
   ],
   "source": [
    "# Get the VG-R dataset\n",
    "vgr_dataset = VG_Relation(image_preprocess=preprocess, download=False, root_dir=root_dir)\n",
    "vgr_loader = DataLoader(vgr_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "# Compute the scores for each test case\n",
    "vgr_scores = model.get_retrieval_scores_batched(vgr_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0da0737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VG-Relation Macro Accuracy: 0.5947169407014137\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the macro accuracy\n",
    "vgr_records = vgr_dataset.evaluate_scores(vgr_scores)\n",
    "symmetric = ['adjusting', 'attached to', 'between', 'bigger than', 'biting', 'boarding', 'brushing', 'chewing', 'cleaning', 'climbing', 'close to', 'coming from', 'coming out of', 'contain', 'crossing', 'dragging', 'draped over', 'drinking', 'drinking from', 'driving', 'driving down', 'driving on', 'eating from', 'eating in', 'enclosing', 'exiting', 'facing', 'filled with', 'floating in', 'floating on', 'flying', 'flying above', 'flying in', 'flying over', 'flying through', 'full of', 'going down', 'going into', 'going through', 'grazing in', 'growing in', 'growing on', 'guiding', 'hanging from', 'hanging in', 'hanging off', 'hanging over', 'higher than', 'holding onto', 'hugging', 'in between', 'jumping off', 'jumping on', 'jumping over', 'kept in', 'larger than', 'leading', 'leaning over', 'leaving', 'licking', 'longer than', 'looking in', 'looking into', 'looking out', 'looking over', 'looking through', 'lying next to', 'lying on top of', 'making', 'mixed with', 'mounted on', 'moving', 'on the back of', 'on the edge of', 'on the front of', 'on the other side of', 'opening', 'painted on', 'parked at', 'parked beside', 'parked by', 'parked in', 'parked in front of', 'parked near', 'parked next to', 'perched on', 'petting', 'piled on', 'playing', 'playing in', 'playing on', 'playing with', 'pouring', 'reaching for', 'reading', 'reflected on', 'riding on', 'running in', 'running on', 'running through', 'seen through', 'sitting behind', 'sitting beside', 'sitting by', 'sitting in front of', 'sitting near', 'sitting next to', 'sitting under', 'skiing down', 'skiing on', 'sleeping in', 'sleeping on', 'smiling at', 'sniffing', 'splashing', 'sprinkled on', 'stacked on', 'standing against', 'standing around', 'standing behind', 'standing beside', 'standing in front of', 'standing near', 'standing next to', 'staring at', 'stuck in', 'surrounding', 'swimming in', 'swinging', 'talking to', 'topped with', 'touching', 'traveling down', 'traveling on', 'tying', 'typing on', 'underneath', 'wading in', 'waiting for', 'walking across', 'walking by', 'walking down', 'walking next to', 'walking through', 'working in', 'working on', 'worn on', 'wrapped around', 'wrapped in', 'by', 'of', 'near', 'next to', 'with', 'beside', 'on the side of', 'around']\n",
    "df = pd.DataFrame(vgr_records)\n",
    "df = df[~df.Relation.isin(symmetric)]\n",
    "print(f\"VG-Relation Macro Accuracy: {df.Accuracy.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fe7ced3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing retrieval scores: 100%|██████████| 1797/1797 [05:33<00:00,  5.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get the VG-A dataset\n",
    "vga_dataset = VG_Attribution(image_preprocess=preprocess, download=False, root_dir=root_dir)\n",
    "vga_loader = DataLoader(vga_dataset, batch_size=16, shuffle=False)\n",
    "# Compute the scores for each test case\n",
    "vga_scores = model.get_retrieval_scores_batched(vga_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "760c2ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VG-Attribution Macro Accuracy: 0.6284264294250497\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the macro accuracy\n",
    "vga_records = vga_dataset.evaluate_scores(vga_scores)\n",
    "df = pd.DataFrame(vga_records)\n",
    "print(f\"VG-Attribution Macro Accuracy: {df.Accuracy.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c974373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
