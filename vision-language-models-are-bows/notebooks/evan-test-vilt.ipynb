{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35150174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from model_zoo import get_model\n",
    "from dataset_zoo import VG_Relation, VG_Attribution\n",
    "\n",
    "from transformers import ViltProcessor, ViltForImageAndTextRetrieval\n",
    "\n",
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02d2d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please put your data root directory below. We'll download VG-Relation and VG-Attribution images here. \n",
    "# Will be a 1GB zip file (a subset of GQA).\n",
    "root_dir=\"C:/Users/ewang/OneDrive/Desktop/Fall 2023/CompVLMs/vision-language-models-are-bows/data2\" \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, preprocess = get_model(\n",
    "        model_name=\"../model_zoo/vilt-b32-finetuned-coco\",\n",
    "        device=device,\n",
    "        root_dir=root_dir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9f98a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_retrieval_scores_batched(joint_loader):\n",
    "    \"\"\"Computes the scores for each image_option / caption_option pair in the joint loader.\n",
    "\n",
    "    Args:\n",
    "        joint_loader (DataLoader): batches have \"image_options\" and \"caption_options\" fields.\n",
    "        \"image_options\" is a list of images, and \"caption_options\" is a list of captions.\n",
    "\n",
    "    Returns:\n",
    "        all_scores: A numpy array containing the scores of the shape NxKxL,\n",
    "        where N is the number of test cases, K is the number of image options per the test case,\n",
    "        and L is the number of caption options per the test case.\n",
    "    \"\"\"\n",
    "\n",
    "    tqdm_loader = tqdm(joint_loader)\n",
    "    tqdm_loader.set_description(\"Computing retrieval scores\")\n",
    "\n",
    "    scores = []\n",
    "    # Iterate over the batched data\n",
    "    for batch in tqdm_loader:\n",
    "        batch_scores = []\n",
    "        i_options = batch[\"image_options\"][0]\n",
    "        c_options0 = batch[\"caption_options\"][0]\n",
    "        c_options1 = batch[\"caption_options\"][1]\n",
    "        for i in range(len(i_options)):\n",
    "            inputs0 = model.processor(\n",
    "                images=i_options[i],\n",
    "                text=c_options0[i],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "            )\n",
    "            inputs1 = model.processor(\n",
    "                images=i_options[i],\n",
    "                text=c_options1[i],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "            )\n",
    "\n",
    "            outputs0 = model.model(**inputs0)\n",
    "            outputs1 = model.model(**inputs1)\n",
    "            score0 = outputs0.logits[:, 0].item()\n",
    "            score1 = outputs1.logits[:, 0].item()\n",
    "\n",
    "            batch_scores.append([score0, score1])  # B x L (16x2)\n",
    "\n",
    "        batch_scores = np.expand_dims(batch_scores, axis=1)  # B x K x L (16x1x2)\n",
    "\n",
    "        scores.append(batch_scores)\n",
    "\n",
    "    all_scores = np.concatenate(scores, axis=0)  # N x K x L\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b942fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing retrieval scores: 100%|██████████| 8/8 [01:19<00:00,  9.88s/it]\n"
     ]
    }
   ],
   "source": [
    "# Get the VG-R dataset\n",
    "vgr_dataset = VG_Relation(\n",
    "        image_preprocess=preprocess, download=False, root_dir=root_dir\n",
    "    )\n",
    "\n",
    "subset_size = int(len(vgr_dataset) * 0.005)\n",
    "\n",
    "subset_dataset = Subset(vgr_dataset, np.arange(subset_size))\n",
    "\n",
    "vgr_loader = DataLoader(subset_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Compute the scores for each test case\n",
    "vgr_scores = get_retrieval_scores_batched(vgr_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 119 but corresponding boolean dimension is 23937",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ewang\\OneDrive\\Desktop\\Fall 2023\\CompVLMs\\vision-language-models-are-bows\\notebooks\\evan-test-vilt.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ewang/OneDrive/Desktop/Fall%202023/CompVLMs/vision-language-models-are-bows/notebooks/evan-test-vilt.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Evaluate the macro accuracy\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ewang/OneDrive/Desktop/Fall%202023/CompVLMs/vision-language-models-are-bows/notebooks/evan-test-vilt.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m vgr_records \u001b[39m=\u001b[39m vgr_dataset\u001b[39m.\u001b[39mevaluate_scores(vgr_scores)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ewang/OneDrive/Desktop/Fall%202023/CompVLMs/vision-language-models-are-bows/notebooks/evan-test-vilt.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m symmetric \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39madjusting\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mattached to\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbetween\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbigger than\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbiting\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mboarding\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbrushing\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mchewing\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcleaning\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mclimbing\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mclose to\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcoming from\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcoming out of\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcontain\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcrossing\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdragging\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdraped over\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdrinking\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdrinking from\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdriving\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdriving down\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdriving on\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39meating from\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39meating in\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39menclosing\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mexiting\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfacing\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfilled with\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfloating in\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfloating on\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mflying\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mflying above\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mflying in\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mflying over\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mflying through\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfull of\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgoing down\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgoing into\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgoing through\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgrazing in\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgrowing in\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgrowing on\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mguiding\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhanging from\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhanging in\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhanging off\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhanging over\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhigher than\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mholding onto\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhugging\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39min between\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mjumping off\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mjumping on\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mjumping over\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mkept in\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlarger than\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mleading\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mleaning over\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mleaving\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlicking\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlonger than\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlooking in\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlooking into\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlooking out\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlooking over\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlooking through\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlying next to\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlying on top of\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmaking\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmixed with\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmounted on\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmoving\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mon the back of\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mon the edge of\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mon the front of\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mon the other side of\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mopening\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpainted on\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mparked at\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mparked beside\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mparked by\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mparked in\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mparked in front of\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mparked near\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mparked next to\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mperched on\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpetting\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpiled on\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mplaying\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mplaying in\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mplaying on\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mplaying with\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpouring\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mreaching for\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mreading\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mreflected on\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mriding on\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrunning in\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrunning on\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrunning through\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mseen through\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msitting behind\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msitting beside\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msitting by\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msitting in front of\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msitting near\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msitting next to\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msitting under\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mskiing down\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mskiing on\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msleeping in\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msleeping on\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msmiling at\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msniffing\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msplashing\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msprinkled on\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstacked on\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstanding against\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstanding around\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstanding behind\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstanding beside\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstanding in front of\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstanding near\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstanding next to\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstaring at\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstuck in\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msurrounding\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mswimming in\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mswinging\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtalking to\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtopped with\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtouching\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtraveling down\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtraveling on\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtying\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtyping on\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39munderneath\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwading in\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwaiting for\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwalking across\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwalking by\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwalking down\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwalking next to\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwalking through\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mworking in\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mworking on\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mworn on\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwrapped around\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwrapped in\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mby\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mof\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnear\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnext to\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwith\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbeside\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mon the side of\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39maround\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ewang/OneDrive/Desktop/Fall%202023/CompVLMs/vision-language-models-are-bows/notebooks/evan-test-vilt.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(vgr_records)\n",
      "File \u001b[1;32mc:\\Users\\ewang\\OneDrive\\Desktop\\Fall 2023\\CompVLMs\\vision-language-models-are-bows\\notebooks\\..\\dataset_zoo\\aro_datasets.py:164\u001b[0m, in \u001b[0;36mVG_Relation.evaluate_scores\u001b[1;34m(self, scores)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[39mif\u001b[39;00m relation_mask\u001b[39m.\u001b[39msum() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    160\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     result_records\u001b[39m.\u001b[39mappend(\n\u001b[0;32m    162\u001b[0m         {\n\u001b[0;32m    163\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mRelation\u001b[39m\u001b[39m\"\u001b[39m: relation,\n\u001b[1;32m--> 164\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAccuracy\u001b[39m\u001b[39m\"\u001b[39m: correct_mask[relation_mask]\u001b[39m.\u001b[39mmean(),\n\u001b[0;32m    165\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCount\u001b[39m\u001b[39m\"\u001b[39m: relation_mask\u001b[39m.\u001b[39msum(),\n\u001b[0;32m    166\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mVisual Genome Relation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    167\u001b[0m         }\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m \u001b[39mreturn\u001b[39;00m result_records\n",
      "\u001b[1;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 119 but corresponding boolean dimension is 23937"
     ]
    }
   ],
   "source": [
    "# Evaluate the macro accuracy\n",
    "vgr_records = vgr_dataset.evaluate_scores_accuracy(vgr_scores)\n",
    "print(vgr_records[\"Accuracy\"])\n",
    "# symmetric = ['adjusting', 'attached to', 'between', 'bigger than', 'biting', 'boarding', 'brushing', 'chewing', 'cleaning', 'climbing', 'close to', 'coming from', 'coming out of', 'contain', 'crossing', 'dragging', 'draped over', 'drinking', 'drinking from', 'driving', 'driving down', 'driving on', 'eating from', 'eating in', 'enclosing', 'exiting', 'facing', 'filled with', 'floating in', 'floating on', 'flying', 'flying above', 'flying in', 'flying over', 'flying through', 'full of', 'going down', 'going into', 'going through', 'grazing in', 'growing in', 'growing on', 'guiding', 'hanging from', 'hanging in', 'hanging off', 'hanging over', 'higher than', 'holding onto', 'hugging', 'in between', 'jumping off', 'jumping on', 'jumping over', 'kept in', 'larger than', 'leading', 'leaning over', 'leaving', 'licking', 'longer than', 'looking in', 'looking into', 'looking out', 'looking over', 'looking through', 'lying next to', 'lying on top of', 'making', 'mixed with', 'mounted on', 'moving', 'on the back of', 'on the edge of', 'on the front of', 'on the other side of', 'opening', 'painted on', 'parked at', 'parked beside', 'parked by', 'parked in', 'parked in front of', 'parked near', 'parked next to', 'perched on', 'petting', 'piled on', 'playing', 'playing in', 'playing on', 'playing with', 'pouring', 'reaching for', 'reading', 'reflected on', 'riding on', 'running in', 'running on', 'running through', 'seen through', 'sitting behind', 'sitting beside', 'sitting by', 'sitting in front of', 'sitting near', 'sitting next to', 'sitting under', 'skiing down', 'skiing on', 'sleeping in', 'sleeping on', 'smiling at', 'sniffing', 'splashing', 'sprinkled on', 'stacked on', 'standing against', 'standing around', 'standing behind', 'standing beside', 'standing in front of', 'standing near', 'standing next to', 'staring at', 'stuck in', 'surrounding', 'swimming in', 'swinging', 'talking to', 'topped with', 'touching', 'traveling down', 'traveling on', 'tying', 'typing on', 'underneath', 'wading in', 'waiting for', 'walking across', 'walking by', 'walking down', 'walking next to', 'walking through', 'working in', 'working on', 'worn on', 'wrapped around', 'wrapped in', 'by', 'of', 'near', 'next to', 'with', 'beside', 'on the side of', 'around']\n",
    "# df = pd.DataFrame(vgr_records)\n",
    "# df = df[~df.Relation.isin(symmetric)]\n",
    "# print(f\"VG-Relation Macro Accuracy: {df.Accuracy.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Your tensor 'image_tensor' should be of shape (3, 224, 224) if it's an RGB image\n",
    "# image_tensor = torch.randn(3, 224, 224)  # Example tensor, replace with your own\n",
    "\n",
    "def show_tensor_image(image_tensor):\n",
    "    # Check if the tensor is on GPU, and if so, move it back to CPU\n",
    "    if image_tensor.is_cuda:\n",
    "        image_tensor = image_tensor.cpu()\n",
    "\n",
    "    # Convert to NumPy array after transposing the dimensions to (H x W x C)\n",
    "    image_numpy = image_tensor.permute(1, 2, 0).numpy()\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image_numpy)\n",
    "    plt.axis('off')  # Hide the axes\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tensor_image(image_ex[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "if isinstance(vgr_scores, tuple):\n",
    "  print(True)\n",
    "else:\n",
    "  print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Relation': 'above',\n",
       "  'Accuracy': 0.4721189591078067,\n",
       "  'Count': 269,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'adjusting',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'around',\n",
       "  'Accuracy': 0.45454545454545453,\n",
       "  'Count': 22,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'at',\n",
       "  'Accuracy': 0.5866666666666667,\n",
       "  'Count': 75,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'attached to',\n",
       "  'Accuracy': 0.6666666666666666,\n",
       "  'Count': 3,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'behind',\n",
       "  'Accuracy': 0.5609756097560976,\n",
       "  'Count': 574,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'below',\n",
       "  'Accuracy': 0.5645933014354066,\n",
       "  'Count': 209,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'beneath',\n",
       "  'Accuracy': 0.8,\n",
       "  'Count': 10,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'beside',\n",
       "  'Accuracy': 0.42857142857142855,\n",
       "  'Count': 56,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'between',\n",
       "  'Accuracy': 0.5,\n",
       "  'Count': 4,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'bigger than',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'biting',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'boarding',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 2,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'brushing',\n",
       "  'Accuracy': 0.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'by',\n",
       "  'Accuracy': 0.40594059405940597,\n",
       "  'Count': 101,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'carrying',\n",
       "  'Accuracy': 0.3333333333333333,\n",
       "  'Count': 12,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'chewing',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'cleaning',\n",
       "  'Accuracy': 0.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'climbing',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'close to',\n",
       "  'Accuracy': 0.6,\n",
       "  'Count': 5,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'coming from',\n",
       "  'Accuracy': 0.6,\n",
       "  'Count': 5,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'coming out of',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'contain',\n",
       "  'Accuracy': 0.8333333333333334,\n",
       "  'Count': 6,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'covered by',\n",
       "  'Accuracy': 0.5,\n",
       "  'Count': 36,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'covered in',\n",
       "  'Accuracy': 0.7857142857142857,\n",
       "  'Count': 14,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'covered with',\n",
       "  'Accuracy': 0.5625,\n",
       "  'Count': 16,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'covering',\n",
       "  'Accuracy': 0.3939393939393939,\n",
       "  'Count': 33,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'crossing',\n",
       "  'Accuracy': 0.75,\n",
       "  'Count': 8,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'cutting',\n",
       "  'Accuracy': 0.75,\n",
       "  'Count': 12,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'dragging',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'draped over',\n",
       "  'Accuracy': 0.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'drinking',\n",
       "  'Accuracy': 0.0,\n",
       "  'Count': 2,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'drinking from',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'driving',\n",
       "  'Accuracy': 0.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'driving down',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'driving on',\n",
       "  'Accuracy': 0.6666666666666666,\n",
       "  'Count': 6,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'eating',\n",
       "  'Accuracy': 0.5714285714285714,\n",
       "  'Count': 21,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'eating from',\n",
       "  'Accuracy': 0.5,\n",
       "  'Count': 4,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'eating in',\n",
       "  'Accuracy': 0.5,\n",
       "  'Count': 2,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'enclosing',\n",
       "  'Accuracy': 0.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'exiting',\n",
       "  'Accuracy': 0.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'facing',\n",
       "  'Accuracy': 0.3333333333333333,\n",
       "  'Count': 3,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'feeding',\n",
       "  'Accuracy': 0.9,\n",
       "  'Count': 10,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'filled with',\n",
       "  'Accuracy': 0.6666666666666666,\n",
       "  'Count': 6,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'floating in',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 3,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'floating on',\n",
       "  'Accuracy': 0.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'flying',\n",
       "  'Accuracy': 0.5,\n",
       "  'Count': 4,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'flying above',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'flying in',\n",
       "  'Accuracy': 0.42857142857142855,\n",
       "  'Count': 7,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'flying over',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'flying through',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'full of',\n",
       "  'Accuracy': 0.75,\n",
       "  'Count': 4,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'going down',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'going into',\n",
       "  'Accuracy': 0.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'going through',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 2,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'grazing in',\n",
       "  'Accuracy': 0.5,\n",
       "  'Count': 2,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'grazing on',\n",
       "  'Accuracy': 0.1,\n",
       "  'Count': 10,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'growing in',\n",
       "  'Accuracy': 0.6666666666666666,\n",
       "  'Count': 3,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'growing on',\n",
       "  'Accuracy': 0.0,\n",
       "  'Count': 8,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'guiding',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'hanging from',\n",
       "  'Accuracy': 0.3333333333333333,\n",
       "  'Count': 3,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'hanging in',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'hanging off',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 2,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'hanging on',\n",
       "  'Accuracy': 0.7857142857142857,\n",
       "  'Count': 14,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'hanging over',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'higher than',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'holding',\n",
       "  'Accuracy': 0.5704225352112676,\n",
       "  'Count': 142,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'holding onto',\n",
       "  'Accuracy': 0.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'hugging',\n",
       "  'Accuracy': 0.6666666666666666,\n",
       "  'Count': 3,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'in',\n",
       "  'Accuracy': 0.6257062146892656,\n",
       "  'Count': 708,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'in between',\n",
       "  'Accuracy': 0.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'in front of',\n",
       "  'Accuracy': 0.532312925170068,\n",
       "  'Count': 588,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'inside',\n",
       "  'Accuracy': 0.5,\n",
       "  'Count': 58,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'jumping off',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'jumping on',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'jumping over',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 3,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'kept in',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 3,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'larger than',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'leading',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 2,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'leaning against',\n",
       "  'Accuracy': 0.8888888888888888,\n",
       "  'Count': 9,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'leaning on',\n",
       "  'Accuracy': 0.6666666666666666,\n",
       "  'Count': 12,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'leaning over',\n",
       "  'Accuracy': 0.75,\n",
       "  'Count': 8,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'leaving',\n",
       "  'Accuracy': 0.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'licking',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'longer than',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'looking at',\n",
       "  'Accuracy': 0.8387096774193549,\n",
       "  'Count': 31,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'looking in',\n",
       "  'Accuracy': 0.7142857142857143,\n",
       "  'Count': 7,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'looking into',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 3,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'looking out',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 2,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'looking over',\n",
       "  'Accuracy': 0.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'looking through',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'lying in',\n",
       "  'Accuracy': 0.4666666666666667,\n",
       "  'Count': 15,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'lying next to',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'lying on',\n",
       "  'Accuracy': 0.6,\n",
       "  'Count': 60,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'lying on top of',\n",
       "  'Accuracy': 0.5714285714285714,\n",
       "  'Count': 7,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'making',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'mixed with',\n",
       "  'Accuracy': 0.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'mounted on',\n",
       "  'Accuracy': 0.3333333333333333,\n",
       "  'Count': 3,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'moving',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'near',\n",
       "  'Accuracy': 0.4934687953555878,\n",
       "  'Count': 689,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'next to',\n",
       "  'Accuracy': 0.4876847290640394,\n",
       "  'Count': 203,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'of',\n",
       "  'Accuracy': 0.4359673024523161,\n",
       "  'Count': 367,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'on',\n",
       "  'Accuracy': 0.5249406175771971,\n",
       "  'Count': 1684,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'on the back of',\n",
       "  'Accuracy': 0.5,\n",
       "  'Count': 4,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'on the edge of',\n",
       "  'Accuracy': 0.0,\n",
       "  'Count': 2,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'on the front of',\n",
       "  'Accuracy': 0.625,\n",
       "  'Count': 8,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'on the other side of',\n",
       "  'Accuracy': 0.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'on the side of',\n",
       "  'Accuracy': 0.26666666666666666,\n",
       "  'Count': 45,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'on top of',\n",
       "  'Accuracy': 0.4228855721393035,\n",
       "  'Count': 201,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'opening',\n",
       "  'Accuracy': 0.5,\n",
       "  'Count': 2,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'painted on',\n",
       "  'Accuracy': 0.6666666666666666,\n",
       "  'Count': 3,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'parked at',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 4,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'parked beside',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'parked by',\n",
       "  'Accuracy': 0.3333333333333333,\n",
       "  'Count': 3,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'parked in',\n",
       "  'Accuracy': 0.8571428571428571,\n",
       "  'Count': 7,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'parked in front of',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 3,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'parked near',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 2,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'parked next to',\n",
       "  'Accuracy': 0.75,\n",
       "  'Count': 4,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'parked on',\n",
       "  'Accuracy': 0.6666666666666666,\n",
       "  'Count': 21,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'perched on',\n",
       "  'Accuracy': 0.5,\n",
       "  'Count': 2,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'petting',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 2,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'piled on',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'playing',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 2,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'playing in',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 7,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'playing on',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 4,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'playing with',\n",
       "  'Accuracy': 0.5,\n",
       "  'Count': 4,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'pouring',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'pulled by',\n",
       "  'Accuracy': 0.6666666666666666,\n",
       "  'Count': 9,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'pulling',\n",
       "  'Accuracy': 0.3333333333333333,\n",
       "  'Count': 9,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'reaching for',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'reading',\n",
       "  'Accuracy': 0.4,\n",
       "  'Count': 5,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'reflected in',\n",
       "  'Accuracy': 0.6428571428571429,\n",
       "  'Count': 14,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'reflected on',\n",
       "  'Accuracy': 0.0,\n",
       "  'Count': 2,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'resting on',\n",
       "  'Accuracy': 0.46153846153846156,\n",
       "  'Count': 13,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'riding',\n",
       "  'Accuracy': 0.6862745098039216,\n",
       "  'Count': 51,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'riding on',\n",
       "  'Accuracy': 0.75,\n",
       "  'Count': 4,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'running in',\n",
       "  'Accuracy': 0.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'running on',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 3,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'running through',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 2,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'seen through',\n",
       "  'Accuracy': 0.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'sitting at',\n",
       "  'Accuracy': 0.6153846153846154,\n",
       "  'Count': 26,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'sitting behind',\n",
       "  'Accuracy': 0.5,\n",
       "  'Count': 2,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'sitting beside',\n",
       "  'Accuracy': 0.5,\n",
       "  'Count': 2,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'sitting by',\n",
       "  'Accuracy': 0.6666666666666666,\n",
       "  'Count': 3,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'sitting in',\n",
       "  'Accuracy': 0.5652173913043478,\n",
       "  'Count': 23,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'sitting in front of',\n",
       "  'Accuracy': 0.75,\n",
       "  'Count': 4,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'sitting near',\n",
       "  'Accuracy': 0.6666666666666666,\n",
       "  'Count': 3,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'sitting next to',\n",
       "  'Accuracy': 0.75,\n",
       "  'Count': 4,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'sitting on',\n",
       "  'Accuracy': 0.5942857142857143,\n",
       "  'Count': 175,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'sitting on top of',\n",
       "  'Accuracy': 0.5,\n",
       "  'Count': 10,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'sitting under',\n",
       "  'Accuracy': 0.0,\n",
       "  'Count': 2,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'skiing down',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'skiing on',\n",
       "  'Accuracy': 0.5,\n",
       "  'Count': 2,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'sleeping in',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'sleeping on',\n",
       "  'Accuracy': 0.4,\n",
       "  'Count': 5,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'smiling at',\n",
       "  'Accuracy': 0.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'sniffing',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 2,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'splashing',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'sprinkled on',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'stacked on',\n",
       "  'Accuracy': 0.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'standing against',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 3,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'standing around',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'standing behind',\n",
       "  'Accuracy': 0.6666666666666666,\n",
       "  'Count': 6,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'standing beside',\n",
       "  'Accuracy': 0.75,\n",
       "  'Count': 4,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'standing by',\n",
       "  'Accuracy': 0.6666666666666666,\n",
       "  'Count': 12,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'standing in',\n",
       "  'Accuracy': 0.7288135593220338,\n",
       "  'Count': 59,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'standing in front of',\n",
       "  'Accuracy': 0.4,\n",
       "  'Count': 5,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'standing near',\n",
       "  'Accuracy': 0.8571428571428571,\n",
       "  'Count': 7,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'standing next to',\n",
       "  'Accuracy': 0.8571428571428571,\n",
       "  'Count': 7,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'standing on',\n",
       "  'Accuracy': 0.5961538461538461,\n",
       "  'Count': 52,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'staring at',\n",
       "  'Accuracy': 0.5,\n",
       "  'Count': 2,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'stuck in',\n",
       "  'Accuracy': 0.6666666666666666,\n",
       "  'Count': 3,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'surrounded by',\n",
       "  'Accuracy': 0.6428571428571429,\n",
       "  'Count': 14,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'surrounding',\n",
       "  'Accuracy': 0.625,\n",
       "  'Count': 8,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'swimming in',\n",
       "  'Accuracy': 0.5,\n",
       "  'Count': 2,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'swinging',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 6,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'talking to',\n",
       "  'Accuracy': 0.3333333333333333,\n",
       "  'Count': 3,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'to the left of',\n",
       "  'Accuracy': 0.4932179304999354,\n",
       "  'Count': 7741,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'to the right of',\n",
       "  'Accuracy': 0.4908926495284847,\n",
       "  'Count': 7741,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'topped with',\n",
       "  'Accuracy': 0.6666666666666666,\n",
       "  'Count': 3,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'touching',\n",
       "  'Accuracy': 0.14285714285714285,\n",
       "  'Count': 7,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'traveling down',\n",
       "  'Accuracy': 0.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'traveling on',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'tying',\n",
       "  'Accuracy': 0.5,\n",
       "  'Count': 2,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'typing on',\n",
       "  'Accuracy': 0.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'under',\n",
       "  'Accuracy': 0.6363636363636364,\n",
       "  'Count': 132,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'underneath',\n",
       "  'Accuracy': 0.5,\n",
       "  'Count': 6,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'using',\n",
       "  'Accuracy': 0.8421052631578947,\n",
       "  'Count': 19,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'wading in',\n",
       "  'Accuracy': 0.5,\n",
       "  'Count': 2,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'waiting for',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 2,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'walking across',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'walking by',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'walking down',\n",
       "  'Accuracy': 0.5,\n",
       "  'Count': 4,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'walking in',\n",
       "  'Accuracy': 0.7,\n",
       "  'Count': 10,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'walking next to',\n",
       "  'Accuracy': 0.5,\n",
       "  'Count': 2,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'walking on',\n",
       "  'Accuracy': 0.7894736842105263,\n",
       "  'Count': 19,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'walking through',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'watching',\n",
       "  'Accuracy': 0.45454545454545453,\n",
       "  'Count': 22,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'wearing',\n",
       "  'Accuracy': 0.46891464699683877,\n",
       "  'Count': 949,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'with',\n",
       "  'Accuracy': 0.5803571428571429,\n",
       "  'Count': 112,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'working in',\n",
       "  'Accuracy': 0.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'working on',\n",
       "  'Accuracy': 0.8333333333333334,\n",
       "  'Count': 6,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'worn on',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 2,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'wrapped around',\n",
       "  'Accuracy': 1.0,\n",
       "  'Count': 1,\n",
       "  'Dataset': 'Visual Genome Relation'},\n",
       " {'Relation': 'wrapped in',\n",
       "  'Accuracy': 0.0,\n",
       "  'Count': 2,\n",
       "  'Dataset': 'Visual Genome Relation'}]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgr_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d0da0737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VG-Relation Macro Accuracy: 0.5947169407014137\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the macro accuracy\n",
    "vgr_records = vgr_dataset.evaluate_scores(vgr_scores)\n",
    "symmetric = ['adjusting', 'attached to', 'between', 'bigger than', 'biting', 'boarding', 'brushing', 'chewing', 'cleaning', 'climbing', 'close to', 'coming from', 'coming out of', 'contain', 'crossing', 'dragging', 'draped over', 'drinking', 'drinking from', 'driving', 'driving down', 'driving on', 'eating from', 'eating in', 'enclosing', 'exiting', 'facing', 'filled with', 'floating in', 'floating on', 'flying', 'flying above', 'flying in', 'flying over', 'flying through', 'full of', 'going down', 'going into', 'going through', 'grazing in', 'growing in', 'growing on', 'guiding', 'hanging from', 'hanging in', 'hanging off', 'hanging over', 'higher than', 'holding onto', 'hugging', 'in between', 'jumping off', 'jumping on', 'jumping over', 'kept in', 'larger than', 'leading', 'leaning over', 'leaving', 'licking', 'longer than', 'looking in', 'looking into', 'looking out', 'looking over', 'looking through', 'lying next to', 'lying on top of', 'making', 'mixed with', 'mounted on', 'moving', 'on the back of', 'on the edge of', 'on the front of', 'on the other side of', 'opening', 'painted on', 'parked at', 'parked beside', 'parked by', 'parked in', 'parked in front of', 'parked near', 'parked next to', 'perched on', 'petting', 'piled on', 'playing', 'playing in', 'playing on', 'playing with', 'pouring', 'reaching for', 'reading', 'reflected on', 'riding on', 'running in', 'running on', 'running through', 'seen through', 'sitting behind', 'sitting beside', 'sitting by', 'sitting in front of', 'sitting near', 'sitting next to', 'sitting under', 'skiing down', 'skiing on', 'sleeping in', 'sleeping on', 'smiling at', 'sniffing', 'splashing', 'sprinkled on', 'stacked on', 'standing against', 'standing around', 'standing behind', 'standing beside', 'standing in front of', 'standing near', 'standing next to', 'staring at', 'stuck in', 'surrounding', 'swimming in', 'swinging', 'talking to', 'topped with', 'touching', 'traveling down', 'traveling on', 'tying', 'typing on', 'underneath', 'wading in', 'waiting for', 'walking across', 'walking by', 'walking down', 'walking next to', 'walking through', 'working in', 'working on', 'worn on', 'wrapped around', 'wrapped in', 'by', 'of', 'near', 'next to', 'with', 'beside', 'on the side of', 'around']\n",
    "df = pd.DataFrame(vgr_records)\n",
    "df = df[~df.Relation.isin(symmetric)]\n",
    "print(f\"VG-Relation Macro Accuracy: {df.Accuracy.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fe7ced3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing retrieval scores: 100%|██████████| 1797/1797 [05:33<00:00,  5.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get the VG-A dataset\n",
    "vga_dataset = VG_Attribution(image_preprocess=preprocess, download=False, root_dir=root_dir)\n",
    "vga_loader = DataLoader(vga_dataset, batch_size=16, shuffle=False)\n",
    "# Compute the scores for each test case\n",
    "vga_scores = model.get_retrieval_scores_batched(vga_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "760c2ef6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vga_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ewang\\OneDrive\\Desktop\\Fall 2023\\CompVLMs\\vision-language-models-are-bows\\notebooks\\evan-test.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ewang/OneDrive/Desktop/Fall%202023/CompVLMs/vision-language-models-are-bows/notebooks/evan-test.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Evaluate the macro accuracy\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ewang/OneDrive/Desktop/Fall%202023/CompVLMs/vision-language-models-are-bows/notebooks/evan-test.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m vga_records \u001b[39m=\u001b[39m vga_dataset\u001b[39m.\u001b[39mevaluate_scores(vga_scores)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ewang/OneDrive/Desktop/Fall%202023/CompVLMs/vision-language-models-are-bows/notebooks/evan-test.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(vga_records)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ewang/OneDrive/Desktop/Fall%202023/CompVLMs/vision-language-models-are-bows/notebooks/evan-test.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mVG-Attribution Macro Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mdf\u001b[39m.\u001b[39mAccuracy\u001b[39m.\u001b[39mmean()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vga_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate the macro accuracy\n",
    "vga_records = vga_dataset.evaluate_scores(vga_scores)\n",
    "df = pd.DataFrame(vga_records)\n",
    "print(f\"VG-Attribution Macro Accuracy: {df.Accuracy.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c974373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
