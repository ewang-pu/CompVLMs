{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35150174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from model_zoo import get_model\n",
    "from dataset_zoo import VG_Relation, VG_Attribution\n",
    "\n",
    "from transformers import ViltProcessor, ViltForImageAndTextRetrieval\n",
    "\n",
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0094edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vilt_processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-coco\")\n",
    "vilt_model = ViltForImageAndTextRetrieval.from_pretrained(\"dandelin/vilt-b32-finetuned-coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02d2d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please put your data root directory below. We'll download VG-Relation and VG-Attribution images here. \n",
    "# Will be a 1GB zip file (a subset of GQA).\n",
    "root_dir=\"C:/Users/ewang/OneDrive/Desktop/Fall 2023/CompVLMs/vision-language-models-are-bows/data2\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cc4eb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = get_model(model_name=\"openai-clip:ViT-B/32\", device=\"cuda\", root_dir=root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from C:/Users/ewang/OneDrive/Desktop/Fall 2023/CompVLMs/vision-language-models-are-bows/data2\\blip-flickr-base.pth\n",
      "missing keys:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = get_model(model_name=\"blip-flickr-base\", device=\"cuda\", root_dir=root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the NegCLIP model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=1ooVVPxB-tvptgmHlIMMFGV3Cg-IrhbRZ\n",
      "From (redirected): https://drive.google.com/uc?id=1ooVVPxB-tvptgmHlIMMFGV3Cg-IrhbRZ&confirm=t&uuid=8d97cb0d-0d0d-4c76-81f4-786ab809f455\n",
      "To: C:\\Users\\ewang\\OneDrive\\Desktop\\Fall 2023\\CompVLMs\\vision-language-models-are-bows\\data2\\negclip.pth\n",
      "100%|██████████| 1.82G/1.82G [01:18<00:00, 23.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = get_model(model_name=\"NegCLIP\", device=\"cuda\", root_dir=root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9f98a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_retrieval_scores_batched(joint_loader):\n",
    "    \"\"\"Computes the scores for each image_option / caption_option pair in the joint loader.\n",
    "\n",
    "    Args:\n",
    "        joint_loader (DataLoader): batches have \"image_options\" and \"caption_options\" fields.\n",
    "        \"image_options\" is a list of images, and \"caption_options\" is a list of captions.\n",
    "\n",
    "    Returns:\n",
    "        all_scores: A numpy array containing the scores of the shape NxKxL,\n",
    "        where N is the number of test cases, K is the number of image options per the test case,\n",
    "        and L is the number of caption options per the test case.\n",
    "    \"\"\"\n",
    "\n",
    "    global image_ex\n",
    "    global caption_ex\n",
    "    caption_ex = []\n",
    "    scores = []\n",
    "    tqdm_loader = tqdm(joint_loader)\n",
    "    tqdm_loader.set_description(\"Computing retrieval scores\")\n",
    "    for batch in tqdm_loader:\n",
    "\n",
    "        image_options = []\n",
    "        # print(batch[\"image_options\"])\n",
    "        # print(len(batch[\"caption_options\"]))\n",
    "        # print(batch[\"caption_options\"][0])\n",
    "        # print(batch[\"caption_options\"][1])\n",
    "        counter = 0\n",
    "        for i_option in batch[\"image_options\"]: # length 1\n",
    "            # print(i_option)\n",
    "            # print(i_option.shape) # torch.Size([16, 3, 224, 224])\n",
    "            image_embeddings = model.model.encode_image(i_option.to(model.device)).cpu().numpy() # B x D\n",
    "            # print(image_embeddings.shape)\n",
    "            # print(np.expand_dims(image_embeddings, axis=1).shape)\n",
    "            image_embeddings = image_embeddings / np.linalg.norm(image_embeddings, axis=1, keepdims=True) # B x D\n",
    "            image_options.append(np.expand_dims(image_embeddings, axis=1)) # B x 1 x D\n",
    "\n",
    "            # image_ex = i_option\n",
    "\n",
    "        \n",
    "        caption_options = []\n",
    "        # caption_ex = []\n",
    "        for c_option in batch[\"caption_options\"]: # length 2\n",
    "\n",
    "            caption_tokenized = torch.cat([clip.tokenize(c) for c in c_option])\n",
    "            caption_embeddings = model.model.encode_text(caption_tokenized.to(model.device)).cpu().numpy() # B x D\n",
    "            caption_embeddings = caption_embeddings / np.linalg.norm(caption_embeddings, axis=1, keepdims=True) # B x D\n",
    "            caption_options.append(np.expand_dims(caption_embeddings, axis=1)) # B x 2 x D\n",
    "            # caption_ex.append(c_option)\n",
    "\n",
    "\n",
    "        image_options = np.concatenate(image_options, axis=1) # B x K x D\n",
    "        caption_options = np.concatenate(caption_options, axis=1) # B x L x D\n",
    "        batch_scores = np.einsum(\"nkd,nld->nkl\", image_options, caption_options) # B x K x L\n",
    "        # print(batch_scores.shape) # 16 x 1 x 2 (B x K x L)\n",
    "        scores.append(batch_scores)\n",
    "    \n",
    "    all_scores = np.concatenate(scores, axis=0) # N x K x L\n",
    "    # print(all_scores.shape) # (23937, 1, 2)\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_data(joint_loader):\n",
    "    \"\"\"Computes the scores for each image_option / caption_option pair in the joint loader.\n",
    "\n",
    "    Args:\n",
    "        joint_loader (DataLoader): batches have \"image_options\" and \"caption_options\" fields.\n",
    "        \"image_options\" is a list of images, and \"caption_options\" is a list of captions.\n",
    "\n",
    "    Returns:\n",
    "        all_scores: A numpy array containing the scores of the shape NxKxL,\n",
    "        where N is the number of test cases, K is the number of image options per the test case,\n",
    "        and L is the number of caption options per the test case.\n",
    "    \"\"\"\n",
    "\n",
    "    global images\n",
    "    global captions\n",
    "    tqdm_loader = tqdm(joint_loader)\n",
    "    # tqdm_loader.set_description(\"Computing retrieval scores\")\n",
    "\n",
    "    images = []\n",
    "    captions = []\n",
    "\n",
    "\n",
    "    for batch in tqdm_loader:\n",
    "\n",
    "\n",
    "        for i_option in batch[\"image_options\"]: # length 1\n",
    "            images.append[i_option]\n",
    "        for c_option in batch[\"caption_options\"]: # length 2\n",
    "            captions.append(c_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b942fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing retrieval scores: 100%|██████████| 1497/1497 [05:10<00:00,  4.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get the VG-R dataset\n",
    "vgr_dataset = VG_Relation(image_preprocess=preprocess, download=False, root_dir=root_dir)\n",
    "vgr_loader = DataLoader(vgr_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Compute the scores for each test case\n",
    "vgr_scores = get_retrieval_scores_batched(vgr_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Your tensor 'image_tensor' should be of shape (3, 224, 224) if it's an RGB image\n",
    "# image_tensor = torch.randn(3, 224, 224)  # Example tensor, replace with your own\n",
    "\n",
    "def show_tensor_image(image_tensor):\n",
    "    # Check if the tensor is on GPU, and if so, move it back to CPU\n",
    "    if image_tensor.is_cuda:\n",
    "        image_tensor = image_tensor.cpu()\n",
    "\n",
    "    # Convert to NumPy array after transposing the dimensions to (H x W x C)\n",
    "    image_numpy = image_tensor.permute(1, 2, 0).numpy()\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image_numpy)\n",
    "    plt.axis('off')  # Hide the axes\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_ex' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ewang\\OneDrive\\Desktop\\Fall 2023\\CompVLMs\\vision-language-models-are-bows\\notebooks\\evan-test.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ewang/OneDrive/Desktop/Fall%202023/CompVLMs/vision-language-models-are-bows/notebooks/evan-test.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m show_tensor_image(image_ex[\u001b[39m15\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_ex' is not defined"
     ]
    }
   ],
   "source": [
    "show_tensor_image(image_ex[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "if isinstance(vgr_scores, tuple):\n",
    "  print(True)\n",
    "else:\n",
    "  print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d0da0737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VG-Relation Macro Accuracy: 0.5947169407014137\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the macro accuracy\n",
    "vgr_records = vgr_dataset.evaluate_scores(vgr_scores)\n",
    "symmetric = ['adjusting', 'attached to', 'between', 'bigger than', 'biting', 'boarding', 'brushing', 'chewing', 'cleaning', 'climbing', 'close to', 'coming from', 'coming out of', 'contain', 'crossing', 'dragging', 'draped over', 'drinking', 'drinking from', 'driving', 'driving down', 'driving on', 'eating from', 'eating in', 'enclosing', 'exiting', 'facing', 'filled with', 'floating in', 'floating on', 'flying', 'flying above', 'flying in', 'flying over', 'flying through', 'full of', 'going down', 'going into', 'going through', 'grazing in', 'growing in', 'growing on', 'guiding', 'hanging from', 'hanging in', 'hanging off', 'hanging over', 'higher than', 'holding onto', 'hugging', 'in between', 'jumping off', 'jumping on', 'jumping over', 'kept in', 'larger than', 'leading', 'leaning over', 'leaving', 'licking', 'longer than', 'looking in', 'looking into', 'looking out', 'looking over', 'looking through', 'lying next to', 'lying on top of', 'making', 'mixed with', 'mounted on', 'moving', 'on the back of', 'on the edge of', 'on the front of', 'on the other side of', 'opening', 'painted on', 'parked at', 'parked beside', 'parked by', 'parked in', 'parked in front of', 'parked near', 'parked next to', 'perched on', 'petting', 'piled on', 'playing', 'playing in', 'playing on', 'playing with', 'pouring', 'reaching for', 'reading', 'reflected on', 'riding on', 'running in', 'running on', 'running through', 'seen through', 'sitting behind', 'sitting beside', 'sitting by', 'sitting in front of', 'sitting near', 'sitting next to', 'sitting under', 'skiing down', 'skiing on', 'sleeping in', 'sleeping on', 'smiling at', 'sniffing', 'splashing', 'sprinkled on', 'stacked on', 'standing against', 'standing around', 'standing behind', 'standing beside', 'standing in front of', 'standing near', 'standing next to', 'staring at', 'stuck in', 'surrounding', 'swimming in', 'swinging', 'talking to', 'topped with', 'touching', 'traveling down', 'traveling on', 'tying', 'typing on', 'underneath', 'wading in', 'waiting for', 'walking across', 'walking by', 'walking down', 'walking next to', 'walking through', 'working in', 'working on', 'worn on', 'wrapped around', 'wrapped in', 'by', 'of', 'near', 'next to', 'with', 'beside', 'on the side of', 'around']\n",
    "df = pd.DataFrame(vgr_records)\n",
    "df = df[~df.Relation.isin(symmetric)]\n",
    "print(f\"VG-Relation Macro Accuracy: {df.Accuracy.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fe7ced3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing retrieval scores: 100%|██████████| 1797/1797 [05:33<00:00,  5.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get the VG-A dataset\n",
    "vga_dataset = VG_Attribution(image_preprocess=preprocess, download=False, root_dir=root_dir)\n",
    "vga_loader = DataLoader(vga_dataset, batch_size=16, shuffle=False)\n",
    "# Compute the scores for each test case\n",
    "vga_scores = model.get_retrieval_scores_batched(vga_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "760c2ef6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vga_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ewang\\OneDrive\\Desktop\\Fall 2023\\CompVLMs\\vision-language-models-are-bows\\notebooks\\evan-test.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ewang/OneDrive/Desktop/Fall%202023/CompVLMs/vision-language-models-are-bows/notebooks/evan-test.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Evaluate the macro accuracy\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ewang/OneDrive/Desktop/Fall%202023/CompVLMs/vision-language-models-are-bows/notebooks/evan-test.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m vga_records \u001b[39m=\u001b[39m vga_dataset\u001b[39m.\u001b[39mevaluate_scores(vga_scores)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ewang/OneDrive/Desktop/Fall%202023/CompVLMs/vision-language-models-are-bows/notebooks/evan-test.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(vga_records)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ewang/OneDrive/Desktop/Fall%202023/CompVLMs/vision-language-models-are-bows/notebooks/evan-test.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mVG-Attribution Macro Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mdf\u001b[39m.\u001b[39mAccuracy\u001b[39m.\u001b[39mmean()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vga_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate the macro accuracy\n",
    "vga_records = vga_dataset.evaluate_scores(vga_scores)\n",
    "df = pd.DataFrame(vga_records)\n",
    "print(f\"VG-Attribution Macro Accuracy: {df.Accuracy.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c974373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
